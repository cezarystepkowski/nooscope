<!DOCTYPE html>
<html lang="pl">
    <head>
        <meta charset="UTF-8">
        <title>Nooskop ujawniony - manifest</title>
        <style>
            em {
                font-style: normal;
                letter-spacing: 1px;
            }
        </style>
    </head>
    <body>
        <article>
            <header>
                <h1>Nooskop ujawniony – manifest.</h1>
                <h3>Sztuczna inteligencja jako narzędzie ekstraktywizmu wiedzy</h3>
            </header>

            <section>
                <h2>1. Nieco oświecenia w kwestii mechanizacji rozumu</h2>
                <p>Nooskop to kartografia granic sztucznej inteligencji pomyślana jako prowokacja zarówno wobec informatyki, jak i humanistyki. Każda mapa pokazuje tylko część perspektywy, jest sposobem wywołania debaty. Również i ta mapa jest manifestem – manifestem dysydentów SI. Jej głównym celem jest rzucenie wyzwania mistyfikacjom sztucznej inteligencji. Po pierwsze jako technicznej definicji <strong>inteligencji</strong>, a po drugie jako formie politycznej, która miałaby być <strong>autonomiczna</strong> względem społeczeństwa i tego, co ludzkie<a href="#nooscope-ref-01-bottom" id="nooscope-ref-01-top"><sup>1</sup></a>. W wyrażeniu „sztuczna inteligencja” przymiotnik „sztuczna” jest nośnikiem mitu o autonomii technologii – wskazuje na karykaturalne „pozaludzkie umysły”, które samodzielnie reprodukują się <i>in silico</i>, lecz w rzeczywistości przysłania dwa procesy prawdziwej alienacji: rosnącą geopolityczną autonomię firm technologicznych i zanikanie autonomii pracowników na całym świecie. Nowoczesny projekt mechanizacji ludzkiego umysłu w XXI wieku najwyraźniej zmutował w korporacyjny reżim ekstraktywizmu wiedzy i poznawczego kolonializmu<a href="#nooscope-ref-02-bottom" id="nooscope-ref-02-top"><sup>2</sup></a>. Nie ma w tym nic dziwnego, skoro algorytmy uczenia maszynowego są najmocniejszymi algorytmami do kompresowania informacji.</p>
                <p>Celem mapy nooskopu jest sekularyzacja SI – zmiana jej statusu ideologicznego z „inteligentnej maszyny” na narzędzie wiedzy. Zamiast odwoływać się do legend o pozaludzkim poznaniu rozsądniej byłoby uznać uczenie maszynowe za <strong>przyrząd do powiększania wiedzy</strong>, pomagający dostrzegać cechy, wzorce i korelacje w przepastnych przestrzeniach danych poza zasięgiem człowieka. W historii nauki i technologii już się to zdarzało: przyrządy optyczne pełniły taką rolę w dziejach astronomii i medycyny<a href="#nooscope-ref-03-bottom" id="nooscope-ref-03-top"><sup>3</sup></a>. Jeśli wpisać je w tradycję nauki, uczenie maszynowe jest jedynie <strong>nooskopem</strong>, przyrządem służącym do spoglądania na przestrzeń wiedzy i nawigowania po niej (od greckiego <i>skopein</i> „badać, patrzeć” i <i>noos</i> „wiedza”).</p>
                <p>Diagram nooskopu opiera się na pomyśle Gottfrieda Wilhelma Leibniza i sięga do przyrządów optycznych jako analogii dla struktury całego oprzyrządowania uczenia maszynowego. By przedstawić moc swojego <i>calculus ratiocinator</i> i „znaków graficznych lub umownych” (był to pomysł, by zaprojektować numeryczny, uniwersalny język, w którym można by zakodować i rozwiązać wszystkie problemy ludzkiego rozumowania), Leibniz posłużył się analogią do przyrządów powiększania obrazu, takich jak mikroskop i teleskop. Pisał: <cite>„Śmiem twierdzić, że to (wynalezienie znaków graficznych lub umownych — przyp. tłum.) właśnie stanowi ostateczny wysiłek ducha ludzkiego, a kiedy projekt zostanie wykonany, osiągnięcie szczęścia będzie zależało już tylko od ludzi, posiądą bowiem narzędzie, które nie mniej posłuży do wywyższenia rozumu, niż teleskop służy do wzmocnienia wzroku<a href="#nooscope-ref-04-bottom" id="nooscope-ref-04-top"><sup>4</sup></a>”.</cite> Nie chodzi nam o to, by po raz kolejny mówić o opozycji między kulturami jakościowymi i ilościowymi, ale lepiej nie podążać za credo Leibniza. Kontrowersji nie da się przeliczyć w sposób jednoznaczny. Uczenie maszynowe nie jest najwyższą formą inteligencji.</p>
                <p>W przyrządy pomiaru i postrzegania zawsze wbudowane są pewne odchylenia. Tak jak soczewki mikroskopu czy teleskopu nigdy nie mają idealnej gładkości i krzywizny, tak samo <em>logiczne soczewki</em> uczenia maszynowego zawierają usterki i błędy<!-- @todo przypis -->. Zrozumieć uczenie maszynowe i udokumentować jego wpływ na społeczeństwo to zbadać, w jakim stopniu społeczne dane ulegają dyfrakcji i zniekształceniu przez owe soczewki. Jest to temat ogólnie znany jako dyskusja nad błędem systematycznym w SI, ale polityczne implikacje logicznej formy uczenia maszynowego są głębsze. Uczenie maszynowe nie sprowadza nowych wieków ciemnych, lecz wiek zniekształconej racjonalności, w której episteme przyczynowości zostaje zastąpione przez episteme korelacji. Jeszcze ogólniej rzecz biorąc, SI to nowy reżim prawdy, dowodzenia naukowego, społecznej normy i racjonalności, który często przybiera kształt <em>halucynacji statystycznej</em>. Ten diagram-manifest to nasz sposób, by powiedzieć, że SI, król obliczeń (patriarchalna fantazja zmechanizowanej wiedzy, „naczelnego algorytmu” i <em>automatu alfa</em>), jest nagi. Tutaj ukradkiem zaglądamy do jego czarnej skrzynki.</p>
                <figure>
                    <img src="img/Nooscope 01.gif" alt="Emanuele Tesauro, Il canocchiale aristotelico [Teleskop Arystotelesa]">
                    <figcaption>On the the invention of metaphors as instrument of knowledge magnification<!-- @todo tłumaczenie -->. Emanuele Tesauro, Il canocchiale aristotelico [Teleskop Arystotelesa], fronton edycji z 1670 roku, Turyn.</figcaption>
                </figure>
            </section>

            <section>
                <h2>2. Linia montażowa uczenia maszynowego: dane, algorytm, model</h2>
                <p>Historia SI to historia eksperymentów, maszyn, które zawodzą, akademickich sporów i okresów epickiej rywalizacji o finansowanie przez armię, znanych powszechnie jako „zimy SI”<a href="#nooscope-ref-05-bottom" id="nooscope-ref-05-top"><sup>5</sup></a>. Chociaż dziś korporacyjna SI opisuje swą moc językiem czarnej magii i „nadludzkiego poznania”, obecne rozwiązania nadal są w fazie eksperymentalnej<a href="#nooscope-ref-06-bottom" id="nooscope-ref-06-top"><sup>6</sup></a>. SI jest teraz na tym samym etapie co maszyna parowa przed odkryciem praw termodynamiki, koniecznych do wyjaśnienia i kontroli jej pracy. Podobnie dziś istnieją wydajne sieci neuronowe rozpoznające obraz, ale nie ma <em>teorii uczenia się</em>, która wyjaśniałaby, dlaczego tak dobrze działają i dlaczego tak bardzo zawodzą. Jak w przypadku każdego odkrycia, paradygmat uczenia maszynowego budował się powoli, w tym wypadku przez ostatnie pół wieku. Naczelny algorytm nie pojawił się z dnia na dzień. Trwa raczej stopniowe konstruowanie metod obliczeniowych, którym brak jeszcze wspólnego słownika. Dla przykładu, akademickie podręczniki uczenia maszynowego nie posługują się jednakową terminologią. Jak więc naszkicować niezbędną gramatykę uczenia maszynowego, która byłaby zwięzła i przystępna, bez wdawania się w paranoiczną zabawę w definiowanie Ogólnej Inteligencji?<!-- @todo wielkie litery? --></p>
                <p>Jako narzędzie wiedzy uczenie maszynowe składa się z przedmiotu obserwacji (<strong>zbiór treningowy</strong>), przyrządy obserwacji (<strong>algorytm uczący</strong>) i końcowej reprezentacji (<strong>model statystyczny</strong>). Zestawienie tych trzech elementów prezentujemy tu w postaci szemranego i barokowego diagramu uczenia maszynowego, ekstrawagancko nazwanego nooskopem<a href="#nooscope-ref-07-bottom" id="nooscope-ref-07-top"><sup>7</sup></a>. By trzymać się analogii z przyrządami optycznymi: przepływ informacji w uczeniu maszynowym jest jak promień światła wysyłany przez dane treningowe, skompresowany przez algorytm, poddany dyfrakcji i skierowany w stronę świata przez soczewkę modelu statystycznego.</p>
                <p>Diagram nooskopu dąży do tego, by zilustrować dwie strony uczenia maszynowego za jednym zamachem – <strong>to, jak działa, i to, jak zawodzi</strong> – przez wyliczenie jego głównych składników, jak również ukazanie szerokiego zakresu błędów, ograniczeń, przybliżeń, uprzedzeń, usterek, fałszywych rozumowań i słabości, które są wpisane w ten paradygmat<a href="#nooscope-ref-08-bottom" id="nooscope-ref-08-top"><sup>8</sup></a>. Ów podwójny cel podkreśla, że SI nie jest monolitycznym wzorcem racjonalności, ale szemraną architekturą, stworzoną przez zaadaptowanie różnych technik i trików. Poza tym granice SI nie są tylko techniczne, są wybrukowane ludzkimi uprzedzeniami. Na diagramie nooskopu podstawowe komponenty uczenia maszynowego przedstawione są w centrum, <strong>ludzkie uprzedzenia</strong> i interwencje są po lewej, <strong>błędy systematyczne</strong> i ograniczenia techniczne – po prawej stronie. Soczewki symbolizują błędy i przybliżenia, reprezentują kompresję i zniekształcenie przepływu informacji. Całkowity błąd uczenia maszynowego jest reprezentowany przez środkową soczewkę modelu statystycznego, w której dyfrakcji ulega postrzeganie świata.</p>
                <p>Ograniczenia SI są dziś dostrzegane dzięki dyskusji o błędzie systematycznym – wzmocnieniu przez algorytmy dyskryminacji ze względu na płeć, rasę, sprawność fizyczną i klasę społeczną. W przypadku uczenia maszynowego konieczne jest rozróżnienie błędu historycznego, błędu zbioru danych i błędu algorytmu, z których każdy występuje na innym etapie przepływu informacji<a href="#nooscope-ref-09-bottom" id="nooscope-ref-09-top"><sup>9</sup></a>. <strong>Historyczna stronniczość</strong> (albo stronniczość świata<!-- @todo przypis: Stanisław Lem, Wizja lokalna -->) daje się dostrzec w społeczeństwie już przed interwencją technologii. Niemniej naturalizacja tej stronniczości, to znaczy wcielenie po cichu nierówności w neutralną z pozoru technologię, jest sama w sobie szkodliwa<a href="#nooscope-ref-10-bottom" id="nooscope-ref-10-top"><sup>10</sup></a>. Parafrazując Michelle Alexander, Ruha Benjamin nazywa to Nowym Jimem Kodem<!-- @todo potrzebny przypis tłumaczy -->: <cite>„przyjęcie nowych technologii, które odbijają i reprodukują istniejące nierówności, a które są przedstawiane i postrzegane jako bardziej obiektywne czy bardziej postępowe niż dyskryminacyjne systemy z poprzedniej epoki”<a href="#nooscope-ref-11-bottom" id="nooscope-ref-11-top"><sup>11</sup></a>.</cite> <strong>Błąd systematyczny</strong> danych pojawia się w zbiorze podczas przygotowywania danych treningowych przez pracowników. Najdelikatniejsza część tego procesu to oznaczanie danych, w czasie którego stare, konserwatywne taksonomie mogą dawać zniekształcony obraz świata, przekłamywać zróżnicowanie społeczne i wyostrzać społeczne hierarchie (zobacz niżej przykład ImageNetu).</p>
                <p><strong>Stronniczość algorytmu<!-- @todo przypis tłumacza (ang. bias) --></strong> (znana też jako błąd maszynowy, błąd systematyczny albo obciążenie modelu, szczególnie istotne w diagramie nooskopu) wzmacnia jeszcze stronniczość historyczną i stronniczość zbioru danych za sprawą algorytmów uczenia maszynowego. Problem stronniczości wyrasta przede wszystkim z tego, że algorytmy uczenia maszynowego są jednymi z najbardziej skutecznych narzędzi <strong>kompresji danych</strong>, co rodzi problemy z dokładnością, dyfrakcją i utratą informacji<a href="#nooscope-ref-12-bottom" id="nooscope-ref-12-top"><sup>12</sup></a>. Od najdawniejszych czasów algorytm był procedurą natury ekonomicznej, zaprojektowaną po to, by osiągnąć rezultat w jak najmniejszej liczbie kroków i przy zużyciu jak najmniejszej ilości zasobów: czasu, energii i pracy<a href="#nooscope-ref-13-bottom" id="nooscope-ref-13-top"><sup>13</sup></a>. W wyścigu zbrojeń między firmami SI wciąż chodzi o znalezienie najprostszych i najszybszych algorytmów, dzięki którym można spieniężać dane. Chociaż kompresja informacji wytwarza maksymalną stopę zwrotu dla korporacyjnej SI, to ze społecznego punktu widzenia jest źródłem dyskryminacji i utraty różnorodności kulturowej.</p>
                <p>Tak jak społeczne konsekwencje SI są powszechnie ujmowane jako zjawisko stronniczości, tak jej techniczne ograniczenia są znane jako problem <strong>czarnej skrzynki</strong>. Efekt czarnej skrzynki to zjawisko faktycznie występujące w głębokich sieciach neuronowych (które filtrują informację tak bardzo, że nie da się odtworzyć ich łańcucha rozumowania), ale stało się ono utartą wymówką na rzecz opinii, że systemy SI są nie tylko tajemnicze i nieprzeniknione, ale wręcz „pozaludzkie” i że nie poddają się kontroli<a href="#nooscope-ref-14-bottom" id="nooscope-ref-14-top"><sup>14</sup></a>. Efekt czarnej skrzynki to część natury każdego eksperymentalnego urządzenia na wczesnym etapie jego rozwoju (już wspomnieliśmy, działanie maszyny parowej pozostawało przez pewien czas zagadką, nawet po tym, jak pomyślnie przeszła testy). Prawdziwym problemem jest retoryka czarnej skrzynki, bliska uczuciom zakorzenionym w teoriach spiskowych, w których SI to tajemna moc, której nie da się poznać, zgłębić ani politycznie kontrolować.</p>
            </section>

            <section>
                <h2>3. Zbiór treningowy: społeczne źródła inteligencji maszynowej</h2>
                <p>Masowa cyfryzacja, która rozszerzała swój zasięg wraz z internetem w latach dziewięćdziesiątych i rozrosła się wraz z centrami danych w pierwszej dekadzie XXI wieku, udostępniła ogromne zasoby danych, które po raz pierwszy w historii stały się darmowe i były nieuregulowane prawnie. Reżim <em>ekstraktywizmu wiedzy</em> (znany wówczas jako Big Data) stopniowo zaczął wykorzystywać bardzo wydajne algorytmy do wydobywania informacji z ogólnodostępnych źródeł danych, głównie po to, by przewidywać zachowania konsumentów i sprzedawać reklamy. Ekonomia wiedzy przekształciła się w nową formę kapitalizmu, nazywanego przez różnych autorów <em>kapitalizmem kognitywnym</em>, a później <em>kapitalizmem nadzoru</em><a href="#nooscope-ref-15-bottom" id="nooscope-ref-15-top"><sup>15</sup></a>. Zalew informacji w internecie, ogromne centra danych, szybsze mikroprocesory i algorytmy służące do kompresji danych – wszystko to położyło fundamenty pod powstanie monopoli SI w XXI wieku.</p>
                <p>Jakiego rodzaju kulturowym i technicznym przedmiotem jest zbiór danych stanowiący źródło dla SI? Jakość <strong>danych treningowych</strong> to najważniejszy czynnik wpływający na tak zwaną inteligencję wydobywaną przez algorytmy uczenia maszynowego. Należy brać pod uwagę pewien istotny punkt widzenia, by zrozumieć SI jako nooskop. Dane to pierwsze i najważniejsze źródło wartości i informacji. Drugie to algorytmy – to właśnie one są maszynami, które wyliczają model na podstawie wartości i informacji. Dane treningowe nie są jednak nigdy surowe, niezależne ani bezstronne (już same w sobie są „algorytmiczne”)<a href="#nooscope-ref-16-bottom" id="nooscope-ref-16-top"><sup>16</sup></a>. Przycinanie, formatowanie i edycja zbiorów treningowych to przedsięwzięcie pracochłonne i delikatne, prawdopodobnie istotniejsze dla końcowego rezultatu niż parametry techniczne, które kontrolują algorytm uczący. Akt wyboru tego, a nie innego źródła danych to głęboki znak ludzkiej interwencji w dziedzinę sztucznych umysłów.</p>
                <figure>
                    <img src="img/Nooscope_02 Data.svg" alt="Dane">
                </figure>
                <p>Zbiór treningowy to <strong>twór kulturowy</strong>, a nie tylko techniczny. Składa się zwykle z danych wejściowych, które są połączone z idealnymi danymi wyjściowymi, na przykład obrazki z ich opisami, które nazywane są także etykietami lub metadanymi<a href="#nooscope-ref-17-bottom" id="nooscope-ref-17-top"><sup>17</sup></a>. Kanonicznym przykładem może być kolekcja muzealna wraz z jej archiwum, w którym dzieła sztuki są uporządkowane według metadanych, takich jak autor, rok, technika itp. Proces semiotyczny przypisania nazwy lub kategorii do obrazu nigdy nie jest bezstronny; ten proces zostawia kolejny wyraźny ślad wpływu człowieka na ostateczny wynik poznania maszynowego. Zbiór danych treningowych dla uczenia maszynowego powstaje zazwyczaj w następujących krokach:</p>
                <ol>
                    <li>Produkcja – praca lub zjawiska, które wytwarzają informację;</li>
                    <li>Uchwycenie – zakodowanie informacji do formatu danych przy pomocy jakiegoś narzędzia;</li>
                    <li>Formatowanie – uporządkowanie danych w zbiór danych;</li>
                    <li>Etykietowanie – w uczeniu nadzorowanym zaklasyfikowanie danych do kategorii (metadanych).</li>
                </ol>
                <p>Inteligencja maszynowa jest trenowana na ogromnych zbiorach danych, gromadzonych w sposób, który nie jest ani technicznie, ani społecznie bezstronny. Surowe dane nie istnieją, ponieważ są zależne od ludzkiej pracy, danych osobistych i zachowań społecznych, które kumulują się w długich okresach czasu, w rozległych sieciach i z użyciem kontrowersyjnych taksonomii<a href="#nooscope-ref-18-bottom" id="nooscope-ref-18-top"><sup>18</sup></a>. Najważniejsze zbiory danych treningowych dla uczenia maszynowego (MNIST, ImageNet, Labelled Faces in the Wild) mają swój początek w korporacjach, na uczelniach i w agencjach wojskowych globalnej Północy. Przy bliższym spojrzeniu można wyraźnie dostrzec podział pracy, który wdziera się do globalnego Południa za pomocą platform crowdsourcingowych, wykorzystywanych do edycji i walidacji danych<a href="#nooscope-ref-19-bottom" id="nooscope-ref-19-top"><sup>19</sup></a>. Przypowieść o zbiorach <strong>ImageNetu</strong> pokazuje problemy wielu zbiorów danych SI. ImageNet to zbiór treningowy dla uczenia głębokiego, który stał się <i>de facto</i> głównym punktem odniesienia dla algorytmów rozpoznawania obrazów: rewolucja uczenia głębokiego zaczęła się, kiedy Alex Krizhevsky, Ilya Sutskever i Geoffrey Hinton wygrali doroczny konkurs ImageNetu swoją neuronową siecią konwolucyjną AlexNet<a href="#nooscope-ref-20-bottom" id="nooscope-ref-20-top"><sup>20</sup></a>. ImageNet został uruchomiony przez informatyka Fei-Fei Li w 2006 roku<a href="#nooscope-ref-21-bottom" id="nooscope-ref-21-top"><sup>21</sup></a>. Fei-Fei Li miał trzy intuicje dotyczące tego, jak zbudować wiarygodny zbiór danych do rozpoznawania obrazów. Po pierwsze, ściągnąć miliony darmowych obrazów z serwisów sieciowych takich jak Flickr i Google. Po drugie, zaadaptować komputacyjną taksonomię <strong>WordNet</strong> do etykiet obrazów<a href="#nooscope-ref-22-bottom" id="nooscope-ref-22-top"><sup>22</sup></a>. Po trzecie, przekazać pracę polegającą na nadawaniu etykiet milionom obrazów przy pomocy platformy crowdsourcingowej Amazon Mechanical Turk. Na szarym końcu (oraz na końcu taśmy produkcyjnej) anonimowi robotnicy z całego świata dostawali kilka centów zapłaty za wykonanie zadania – przypisanie obrazom setek etykiet na minutę zgodnie z taksonomią WordNetu. W wyniku ich pracy powstał kontrowersyjny konstrukt kulturowy. Zajmująca się SI uczona Kate Crawford i artysta Trevor Paglen ujawnili osad rasistowskich i seksistowskich kategorii w taksonomii ImageNetu (chociażby zakres kategorii „porażka, nieudacznik, przegryw, osoba przegrana” dla stu dowolnych zdjęć ludzi)<a href="#nooscope-ref-23-bottom" id="nooscope-ref-23-top"><sup>23</sup></a>.</p>
                <p>Żarłoczny ekstraktywizm danych dla SI wywołał niespodziewaną reakcję w kulturze cyfrowej: na początku XXI wieku Lawrence Lessig nie mógł przewidzieć, że wielkie repozytorium obrazów dostępnych online na licencji <strong>Creative Commons</strong> dziesięć lat później stanie się nieuregulowanym prawnie zasobem dla technologii rozpoznawania twarzy służących nadzorowi. W podobny sposób dane osobiste są stale wcielane w prywatne zbiory danych dla uczenia maszynowego. W 2019 artysta i badacz SI Adam Harvey po raz pierwszy ujawnił odbywające się bez pytania o zgodę użycie prywatnych zdjęć w zbiorach treningowych do rozpoznawania twarzy. Rewelacje Harveya skłoniły Uniwersytet Stanforda, Uniwersytet Duke i Microsoft do wycofania swoich zbiorów danych w atmosferze <strong>wielkiego skandalu</strong><a href="#nooscope-ref-24-bottom" id="nooscope-ref-24-top"><sup>24</sup></a>. Dostępne online zbiory danych zmuszają do zadawania pytań o suwerenność danych i o prawa obywatelskie, a reakcja tradycyjnych instytucji jest w tej sprawie bardzo powolna (zobacz europejskie Ogólne rozporządzenie o ochronie danych)<a href="#nooscope-ref-25-bottom" id="nooscope-ref-25-top"><sup>25</sup></a>. O ile 2012 to rok, w którym zaczęła się rewolucja uczenia głębokiego, o tyle 2019 jest rokiem, w którym odkryto, że jego źródła są zatrute.</p>
                <figure>
                    <img src="img/Nooscope-03.gif" alt="Combinatorial patterns and Kufic scripts, Topkapi scroll, ca. 1500, Iran."><!-- @todo tłumaczenie -->
                    <figcaption>Combinatorial patterns and Kufic scripts, Topkapi scroll, ca. 1500, Iran.<!-- @todo tłumaczenie --></figcaption>
                </figure>
            </section>

            <section>
                <h2>4. Historia SI jako automatyzacji postrzegania</h2>
                <p>Potrzeba odczarowania SI (przynajmniej z technicznego punktu widzenia) została zauważona również w świecie korporacji. Szef Facebook AI i ojciec chrzestny konwolucyjnych sieci neuronowych Yann LeCun powtarza, że współczesne systemy SI to wyrafinowane wersje nie zdolności poznawczych, ale percepcji. Podobnie diagram Nooskopu eksponuje szkielet czarnej skrzynki SI i pokazuje, że SI nie jest myślącą maszyną, tylko algorytmem, który <strong>rozpoznaje wzorce</strong>. Skoro wspomnieliśmy o rozpoznawaniu wzorców, to musimy rozwinąć kilka kwestii. Czym w ogóle jest wzorzec? Czy wzorzec jest bytem wyłącznie wizualnym? Co to znaczy odczytywać zachowania społeczne jako wzorce? Czy rozpoznawanie wzorców to wyczerpująca definicja inteligencji? Najprawdopodobniej nie. Aby wyjaśnić te kwestie, dobrze będzie przedsięwziąć krótką wyprawę do archeologii SI.</p>
                <p>Archetypem maszyny do rozpoznawania obrazu jest <strong>perceptron</strong> Franka Rosenblatta. Został on wynaleziony w 1957 roku w Laboratorium Aeronautycznym Cornell w Buffalo w stanie Nowy Jork, a jego nazwa to skrót od „automat percepcyjny i rozpoznający” (Perceiving and Recognizing Automaton)<a href="#nooscope-ref-26-bottom" id="nooscope-ref-26-top"><sup>26</sup></a>. Dysponując matrycą składającą się z 20×20 fotoreceptorów, perceptron może nauczyć się rozpoznawać proste litery. Wzorzec wizualny zostaje zapisany jako ślad na sieci sztucznych neuronów, które wraz z pojawianiem się podobnych obrazów przesyłają impuls i w efekcie aktywują pojedynczy neuron na wyjściu. Neuron na wyjściu przesyła 1=prawda, jeśli dany obraz został rozpoznany, lub 0=fałsz, jeśli obraz nie został rozpoznany.</p>
                <p>Automatyzacja postrzegania, rozumiana jako wizualny montaż pikseli na obliczeniowej taśmie produkcyjnej, była pierwotnie wpisana implicite w wizję sztucznych sieci neuronowych McCullocha i Pittsa<a href="#nooscope-ref-27-bottom" id="nooscope-ref-27-top"><sup>27</sup></a>. Kiedy algorytm rozpoznawania schematów wizualnych przetrwał „zimę SI” i udowodnił swoją wydajność pod koniec pierwszej dekady XXI wieku, znalazł zastosowanie również w niewizualnych zbiorach danych, co właściwie zapoczątkowało wiek uczenia głębokiego (użycie techniki rozpoznawania wzorców do wszystkich rodzajów danych, nie tylko wizualnych). Dziś w przypadku samochodów autonomicznych wzorce, jakie musi rozpoznać maszyna, to obiekty w scenariuszu drogowym. W przypadku automatycznego tłumaczenia rozpoznawane wzorce to najczęstsze sekwencje wyrazów w tekstach dwujęzycznych. Z perspektywy numerycznej wszystkie rzeczy takie jak obraz, ruch, kształt, styl i decyzja etyczna, niezależnie od ich złożoności, można opisać jako rozkład statystyczny wzorca. W tym sensie rozpoznawanie wzorców stało się nową <strong>techniką kulturową</strong> stosowaną w różnych dziedzinach. Na potrzeby wyjaśnienia nooskop został opisany jako maszyna operująca w trzech modalnościach: <strong>trenowanie</strong>, <strong>klasyfikacja</strong> i <strong>predykcja</strong>. Bardziej intuicyjnie można nazwać te modalności: wydobywanie wzorców, rozpoznawanie wzorców, generowanie wzorców. </p>
                <p>Perceptron Rosenblatta był pierwszym algorytmem, który wytyczył drogę uczeniu maszynowemu, jakie znamy dzisiaj. Zanim ugruntowało się określenie „informatyka”, dziedzina ta była nazywana „geometrią obliczeniową”, a przez samego Rosenblatta – „konekcjonizmem”. Zadaniem tamtych sieci neuronowych było obliczanie inferencji statystycznej. Sieć neuronowa nie oblicza samego wzorca, tylko jego <strong>rozkład statystyczny</strong>. Prześlizgując się zaledwie po powierzchni antropomorficznego marketingu SI, można znaleźć jeszcze jeden techniczny i kulturowy obiekt, który warto przebadać: <strong>model statystyczny</strong>. Czym jest model statystyczny w uczeniu maszynowym? Jak się go oblicza? Jaka jest relacja między modelem statystycznym a ludzkim poznaniem? To kluczowe kwestie, które należy wyjaśnić. W ramach pracy demistyfikacyjnej, jaką należy wykonać (również po to, by pozbyć się pewnych naiwnych pytań), dobrze byłoby przeformułować oklepane pytanie „Czy maszyna może myśleć?” na bardziej rozsądne z punktu widzenia teorii pytania: „Czy model statystyczny może myśleć?”, „Czy model statystyczny może rozwinąć świadomość?” itp.</p>
            </section>

            <section>
                <h2>5. Algorytm uczący: kompresowanie świata do modelu statystycznego</h2>
            </section>

            <section>
                <h2>6. Wszystkie modele są błędne, ale niektóre są użyteczne</h2>
            </section>

            <section>
                <h2>7. Świat na wektor</h2>
            </section>

            <section>
                <h2>8. Społeczeństwo klasyfikujących i przewidujących botów</h2>
            </section>

            <section>
                <h2>9. Błędy narzędzia statystycznego: niewykrywanie tego, co nowe</h2>
            </section>

            <section>
                <h2>10. Inteligencja przeciwstawna vs. sztuczna inteligencja</h2>
            </section>

            <section>
                <h2>11. Praca w czasach SI</h2>
            </section>
        </article>
    </body>
</html>
